{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f102c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Analysis for Customer Segmentation\n",
    "\n",
    "# Alejandro Pineda, PhD\n",
    "\n",
    "\n",
    "# Methods: KModes clustering (categorical variables), Shapley values (feature importance), Centroid analysis (profiles)\n",
    "\n",
    "\n",
    "### lookup OPTICS on sklearn to measure density.\n",
    "\n",
    "# Load Libraries (install as necessary)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist \n",
    "#from sklearn import StandardScalar\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from kmodes.kmodes import KModes\n",
    "# seed # (might be possible to set numpy seed globally, but not advised? Investigate further)\n",
    "rando_n = 1234\n",
    "# from lightgbm import LGBMClassifier\n",
    "# import shap\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from plotly import graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "\n",
    "from prince import FAMD\n",
    "import sklearn\n",
    "import altair as alt \n",
    "alt.data_transformers.disable_max_rows()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(file_name, col_drops):\n",
    "    \"\"\"\n",
    "    Loads data//drops columns where necessary//provides shape and dtype info\n",
    "    \n",
    "    \"\"\"\n",
    "    original_dat = pd.read_csv(file_name, low_memory=False)\n",
    "    og_shape = original_dat.shape\n",
    "    \n",
    "    dat_dropped = original_dat.drop(columns=col_drops, errors = 'ignore') # see if we can do try/except here\n",
    "    drop_shape = dat_dropped.shape\n",
    "    \n",
    "    dat_types = pd.DataFrame(dat_dropped.dtypes)\n",
    "    print(\"Header for new data:\")\n",
    "    print(dat_dropped.head())\n",
    "    print(f\"Shape of original data: {og_shape}\" )\n",
    "    print(f\"Shape of new data: {drop_shape}\")\n",
    "\n",
    "    \n",
    "    return dat_dropped, drop_shape, dat_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of our column names have weird names or tags \n",
    "# (legacy data will use tags to tell use what database it came from)\n",
    "# use reg-ex to pattern match and clean\n",
    "\n",
    "# this function needs work\n",
    "\n",
    "def pattern_find(columns, pattern=str):\n",
    "    \"\"\"\n",
    "    Looks for columns that match a specific pattern\n",
    "    \"\"\"\n",
    "    reg_pattern = \".*\" + pattern\n",
    "    display(f\"Looking for the following pattern: {reg_pattern}\")\n",
    "    \n",
    "    r = re.compile(reg_pattern)\n",
    "    col_list = list(filter(r.match, columns)) \n",
    "    display(col_list[0:5])\n",
    "    return col_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing data and identifying categorical columns by index\n",
    "\n",
    "\n",
    "def prep_dat(data_frame, thresh=None, mean_fill = False, dat_fill=None, limit=None, na_drop=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Fills in and/or drops missing values. \n",
    "    \n",
    "    dat_fill (str): ‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None (default)\n",
    "    limit (int): the maximum number of consecutive NaN allowed\n",
    "    na_drop (str): Do you want rows or columns dropped? 'columns','index', or None (default) \n",
    "    thresh (int): Do you want to just require a specific # of non-NA values?\n",
    "    mean_fill: Do you want missing values in numerical columns filled in with column mean? Boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    data_frame2 = data_frame\n",
    "    \n",
    "    num_cols = data_frame2.select_dtypes(exclude=['object']).columns\n",
    "    data_frame2[num_cols] = data_frame2[num_cols].astype(np.float32)\n",
    "    \n",
    "    # handle numerical missing nulls with the mean\n",
    "    if mean_fill:\n",
    "        num_features = list(num_cols)\n",
    "        data_frame2[num_features]  = data_frame2[num_features].fillna(data_frame2.mean())\n",
    "    \n",
    "    # handle categorical missing nulls with the specified method\n",
    "    if dat_fill:\n",
    "        data_frame2 = data_frame2.fillna(method=dat_fill, limit = limit)\n",
    "    \n",
    "    if na_drop:  \n",
    "        data_frame2 = data_frame2.dropna(axis=na_drop, thresh=thresh)\n",
    "    \n",
    "    display(f\"Data loaded for this model:\")\n",
    "    display(data_frame.head())\n",
    "    \n",
    "    display(\"Data after pre-processing: \")\n",
    "    num_features = list(data_frame2.select_dtypes(exclude=['object']).columns)\n",
    "    data_frame2[num_features] = StandardScaler().fit_transform(data_frame2[num_features])\n",
    "    \n",
    "    \n",
    "    # Identifying categorical variables based on index\n",
    "    all_cols = list(data_frame2.columns)\n",
    "    \n",
    "    cat_col_names = list(set(all_cols) - set(num_features))\n",
    "    cat_index = list()\n",
    "    for col in cat_col_names:\n",
    "        # find the index no\n",
    "        index_loc = data_frame2.columns.get_loc(col)\n",
    "        cat_index.append(index_loc)\n",
    "\n",
    "    display(f\"n_categorical features: {len(cat_index)}\")\n",
    "    display(f\"n_numerical features: {len(num_features)}\")\n",
    "    \n",
    "    display(f\"Shape of the original data set: {data_frame.shape} // Shape of new data: {data_frame2.shape}\")\n",
    "    display(\"Pre-processed data:\")\n",
    "    display(data_frame2.head())\n",
    "    return data_frame2\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f571e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we know we won't need\n",
    "# either the columns represent duplicate information or are just useless\n",
    "\n",
    "\n",
    "# lil_trash = ['IndividualID','FAME-Hip Hop']\n",
    "\n",
    "# display(\"Number of trash columns: \", len(lil_trash))\n",
    "\n",
    "\n",
    "# folder = 'C:/OneDrive/OneDrive - YRBrands/Documents/data/Ford_Owner/Ford Owner/'\n",
    "# folder = 'C:/OneDrive/OneDrive - YRBrands/Documents/data/'\n",
    "# folder = 'C:/Users/Alejandro.Pineda/Downloads/May24_Ford_Data/'\n",
    "folder = \"C:/Users/Alejandro.Pineda/Downloads/\"\n",
    "\n",
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e847f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "filename1 = folder +  'data_file.csv' \n",
    "\n",
    "display(filename1)\n",
    "\n",
    "dat1 = pd.read_csv(filename1, header=0)\n",
    "\n",
    "display(dat1.head(100))\n",
    "display(dat1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8018fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat1 = dat1.sample(frac=1, replace=False, random_state=rando_n)\n",
    "dat1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd2b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dat1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adf25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_df = dat1.sample(frac=.20, replace=False, random_state=rando_n)\n",
    "#subset_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = \"path/to/output/folder\"\n",
    "out_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aedf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = dat1.dropna(axis = 'rows', how='any')\n",
    "#dat3 = dat2.dropna(axis = 'columns', thresh = .75)\n",
    "\n",
    "#dat1 = subset_df\n",
    "dat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d8e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(dat1.columns)\n",
    "trash = list()\n",
    "\n",
    "for i in cols:\n",
    "    string = i.lower()\n",
    "    if 'id' in string:\n",
    "        print(i)\n",
    "        trash.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trash.remove('DMA_ID')\n",
    "#trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d4198",
   "metadata": {},
   "outputs": [],
   "source": [
    "trash = ['cols_we_dont_need']\n",
    "\n",
    "\n",
    "dat1.drop(columns = trash, inplace = True)\n",
    "dat1.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0762ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values (data quality is important!)\n",
    "\n",
    "#dat = dat.dropna(axis=1)\n",
    "nulls = dat1.isnull().agg('sum')\n",
    "nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat1.dropna(axis = 'rows', how='any', inplace=True)\n",
    "#nulls = dat3.isnull().agg('sum')\n",
    "#nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32297edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = dat1.dtypes.to_dict()\n",
    "dtype_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95aec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = list(dat2.select_dtypes(exclude=['object']).columns)\n",
    "\n",
    "# dat2.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "\n",
    "# dat3 is downsampled from dat2\n",
    "\n",
    "# dat3 = dat2.sample(n=50000, random_state = rando_n)\n",
    "\n",
    "# dat4 is set aside for output, preserving original values\n",
    "# dat4 = dat3\n",
    "\n",
    "# dat3 gets scaled\n",
    "#num_features = list(dat2.select_dtypes(exclude=['object']).columns)\n",
    "#dat2[num_features] = StandardScaler().fit_transform(dat2[num_features])\n",
    "#dat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2874fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat_prepped = prep_dat(dat2, thresh=None, mean_fill = False, dat_fill='ffill', limit=25, na_drop='rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmodes_trainer(data_frame, k, random_state, num_init=3):\n",
    "    \n",
    "    proto_dat = data_frame\n",
    "        \n",
    "    #### Initialize and train model on give df ####    \n",
    "    \n",
    "    # Leaving init and verbose as settings we don't need to worry about for now\n",
    "\n",
    "    km = KModes(n_clusters= k, init='Cao', verbose=2, random_state=random_state, n_init = num_init)\n",
    "    display(f\"Training k-modes model with {k} clusters and {num_init} centroid inits.\")\n",
    "    \n",
    "    \n",
    "    clusters = km.fit_predict(proto_dat)\n",
    "    proto_dat['label'] = km.predict(proto_dat)\n",
    "    data_frame_lab = proto_dat\n",
    "    \n",
    "    return km, clusters, data_frame_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87494466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kproto_trainer(data_frame, k, random_state, num_init=3):\n",
    "    \n",
    "    proto_dat = data_frame\n",
    "    \n",
    "    num_features = list(proto_dat.select_dtypes(exclude=['object']).columns)\n",
    "    all_cols = list(proto_dat.columns)\n",
    "    \n",
    "    cat_col_names = list(set(all_cols) - set(num_features))\n",
    "    cat_index = list()\n",
    "    for col in cat_col_names:\n",
    "        # find the index no\n",
    "        index_loc = proto_dat.columns.get_loc(col)\n",
    "        cat_index.append(index_loc)\n",
    "    \n",
    "    \n",
    "    #### Initialize and train model on give df ####    \n",
    "    \n",
    "    # Leaving init and verbose as settings we don't need to worry about for now\n",
    "    kproto = KPrototypes(n_clusters=k, init='Huang', verbose=2, random_state=random_state, n_init = num_init)\n",
    "    display(f\"Training k-prototype model with {k} clusters and {num_init} centroid inits.\")\n",
    "    \n",
    "    \n",
    "    clusters = kproto.fit_predict(proto_dat, categorical=cat_index)\n",
    "    proto_dat['label'] = kproto.predict(proto_dat, categorical = cat_index)\n",
    "    data_frame_lab = proto_dat\n",
    "    \n",
    "    return kproto, clusters, data_frame_lab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_dat = dat1.sample(frac=.15, replace=False, random_state=rando_n)\n",
    "proto_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a6de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "num_features = list(proto_dat.select_dtypes(exclude=['object']).columns)\n",
    "all_cols = list(proto_dat.columns)\n",
    "    \n",
    "cat_col_names = list(set(all_cols) - set(num_features))\n",
    "cat_index = list()\n",
    "\n",
    "for col in cat_col_names:\n",
    "    # find the index no\n",
    "    index_loc = proto_dat.columns.get_loc(col)\n",
    "    cat_index.append(index_loc)\n",
    "    \n",
    "costs = []\n",
    "n_clusters = []\n",
    "clusters_assigned = []\n",
    "\n",
    "for i in tqdm(range(2, 9)):\n",
    "    try:\n",
    "        kproto = KPrototypes(n_clusters= i, init='Cao', verbose=2)\n",
    "        clusters = kproto.fit_predict(proto_dat, categorical = cat_index)\n",
    "        costs.append(kproto.cost_)\n",
    "        n_clusters.append(i)\n",
    "        clusters_assigned.append(clusters)\n",
    "    except:\n",
    "        print(f\"Can't cluster with {i} clusters\")\n",
    "        \n",
    "fig = go.Figure(data=go.Scatter(x=n_clusters, y=costs ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow plot with cost (will take a LONG time)\n",
    "  \n",
    "costs = []\n",
    "n_clusters = []\n",
    "clusters_assigned = []\n",
    "\n",
    "for i in tqdm(range(2, 9)):\n",
    "    try:\n",
    "        km = KModes(n_clusters= i, init='Cao', verbose=2)\n",
    "        clusters = km.fit_predict(proto_dat)\n",
    "        costs.append(km.cost_)\n",
    "        n_clusters.append(i)\n",
    "        clusters_assigned.append(clusters)\n",
    "    except:\n",
    "        print(f\"Can't cluster with {i} clusters\")\n",
    "        \n",
    "fig = go.Figure(data=go.Scatter(x=n_clusters, y=costs ))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "# three clusters going forward (looking at elbow plot above)\n",
    "\n",
    "km_model, km_cluster, lab_df = kmodes_trainer(data_frame=proto_dat, random_state=rando_n, k=3, num_init=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d16d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model.cluster_centroids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce69d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure this parallels output file name\n",
    "print(out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3571ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "model_file = \"demographics_query.pkl\"\n",
    "\n",
    "output_file_name = out_folder + model_file\n",
    "output_file_name\n",
    "\n",
    "\n",
    "pickle.dump(km_model, open(output_file_name, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb64750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(output_file_name, 'rb') as file:  \n",
    "    pickled_km = pickle.load(file)\n",
    "\n",
    "pickled_km\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a09826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analysis: tell a story, know your audience, give them the magic***\n",
    "\n",
    "\n",
    "\n",
    "***explain the magic in appendices and be ready to answer any technical questions, i.e., why you did something. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample and verify that the index matches up before stapling the column back\n",
    "#dat3 = dat2.sample(n=50000, random_state = rando_n)\n",
    "#display(dat3.head())\n",
    "#display(lab_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure this parallels output file name\n",
    "print(filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc8188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname = out_folder + \"clusters_renewal_may24.csv\"\n",
    "#lab_df = pd.read_csv(fname)\n",
    "#lab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d12198a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dat3['label'] = lab_df['label']\n",
    "#display(dat2.head(50))\n",
    "#display(lab_df)\n",
    "\n",
    "fname = \"clusters_demographics_military.csv\"\n",
    "out_file = out_folder + fname\n",
    "display(out_file)\n",
    "\n",
    "lab_df.to_csv(out_file, index = False)\n",
    "#lab_df = pd.read_csv(out_file)\n",
    "#lab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_file = \"clusters_nonluxury_midsize_pickup.csv\"\n",
    "\n",
    "\n",
    "output_file_name = out_folder + model_file\n",
    "output_file_name\n",
    "\n",
    "lab_df.to_csv(output_file_name, index=False)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b62f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cluster centroids of the trained model.\n",
    "print(pickled_km.cluster_centroids_)\n",
    "\n",
    "# Print training statistics\n",
    "#print(pickled_km.cost_)\n",
    "#print(pickled_km.n_iter_)\n",
    "\n",
    "\n",
    "#kproto_model, kproto_cluster, lab_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_km.cluster_centroids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdcc05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfe203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "famd = FAMD(n_components=2, n_iter=3, check_input=True, engine='sklearn', random_state=rando_n)\n",
    "\n",
    "famd = famd.fit(lab_df.drop('label', axis='columns'))\n",
    "famd.fit(lab_df)\n",
    "famd.transform(lab_df)\n",
    "\n",
    "famd.plot(\n",
    "    lab_df,\n",
    "    x_component=0,\n",
    "    y_component=1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#clx = famd.plot_row_coordinates(lab_df, figsize=(15,10), color_labels = lab_df['cluster'])\n",
    "#clx.get_figure() #.savefig('clusters_822.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9bf0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d2fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grabbing F1 score to see how well groupings based on categories predicts data\n",
    "# https://en.wikipedia.org/wiki/LightGBM\n",
    "\n",
    "\n",
    "proto_labs = list(lab_df['label'])\n",
    "#np.float = float\n",
    "\n",
    "#Setting the objects to category \n",
    "cat_data = lab_df.drop('label', axis='columns')\n",
    "\n",
    "for i in cat_data.select_dtypes(include='object'):\n",
    "    cat_data[i] = cat_data[i].astype('category')\n",
    "\n",
    "clf_kp = lgb.LGBMClassifier()\n",
    "\n",
    "#cv_scores_kp = cross_val_score(clf_kp, cat_data, lab_df, scoring='f1_weighted')\n",
    "#print(f'CV F1 score for K-Prototypes clusters is {np.mean(cv_scores_kp)}')\n",
    "\n",
    "# So this method is taking a weak learner, a gradient boosted model, to see how well the data maps onto\n",
    "# the cluster labels using the categories\n",
    "#(https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://shap-lrjball.readthedocs.io/en/docs_update/generated/shap.TreeExplainer.html\n",
    "import shap\n",
    "import numba\n",
    "clf_kp.fit(cat_data, proto_labs)\n",
    "explainer_kp = shap.TreeExplainer(clf_kp)\n",
    "shap_values_kp = explainer_kp.shap_values(cat_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(os.listdir(out_folder))\n",
    "display(filename1)\n",
    "\n",
    "plt_title = 'Impactful Features: Military Demographics Query (October 2024)'\n",
    "fname = out_folder + 'shaps_demographics_military.png'\n",
    "display(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_kp, cat_data, color=plt.get_cmap(\"tab10\"), show=False)\n",
    "fig = plt.gcf()\n",
    "fig.set_figheight(12)\n",
    "fig.set_figwidth(25)\n",
    "plt.title(plt_title)\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d79239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat_prepped = prep_dat(lab_df, thresh=None, mean_fill = True, dat_fill='ffill', limit=25, na_drop='columns')\n",
    "lab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6000878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing clusters using scatterplot\n",
    "from sklearn.decomposition import PCA\n",
    "# HIERARCHICAL CLUSTERING\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.cluster import hierarchy as shc\n",
    "import gower\n",
    "import prince\n",
    "# we're using gower's distance because it can capture differences between mixed vars\n",
    "# https://medium.com/analytics-vidhya/gowers-distance-899f9c4bd553\n",
    "\n",
    "df_subsample = lab_df.sample(n=1000, random_state = rando_n)\n",
    "display(df_subsample.head())\n",
    "\n",
    "distance_matrix = gower.gower_matrix(df_subsample)\n",
    "distance_matrix\n",
    "\n",
    "\"\"\"\n",
    "for c in clusters:\n",
    "    grid= sns.FacetGrid(clusters, col='cluster')\n",
    "    grid.map(plt.hist, c)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149daa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = pd.DataFrame(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = df_subsample.label\n",
    "display(filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ca902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot check data name\n",
    "plt_file = out_folder + 'demo_military_scatter.png'\n",
    "display(plt_file)\n",
    "plt_title = 'Demographics Query, Military (October 24)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = prince.MCA(\n",
    "    n_components=2,\n",
    "    n_iter=3,\n",
    "    copy=True,\n",
    "    check_input=True,\n",
    "    engine='sklearn',\n",
    "    random_state=42\n",
    ")\n",
    "mca = mca.fit(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mca.plot(\n",
    "#    distance_matrix,\n",
    "#    x_component=0,\n",
    "#    y_component=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = prince.MCA(2)\n",
    "mca.fit(distance_matrix)\n",
    "X_PCA = mca.transform(distance_matrix)\n",
    "X_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA.loc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d28ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_jitter(arr):\n",
    "    stdev = .01 * (max(arr) - min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev\n",
    "\n",
    "#x = rand_jitter(X_PCA.loc[:,0])\n",
    "#y = rand_jitter(X_PCA.loc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377620f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = labs.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c78975",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c79e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on the distance matrix and then scatterplot to visualize\n",
    "# dope code for future reference: https://www.kaggle.com/code/sabanasimbutt/clustering-visualization-of-clusters-using-pca\n",
    "\n",
    "\n",
    "\n",
    "x, y = X_PCA.loc[:,0], X_PCA.loc[:,1]\n",
    "\n",
    "colors = {0: 'red',\n",
    "          1: 'blue',\n",
    "          2: 'green'}\n",
    "\n",
    "names = {0: 'Cluster 0', \n",
    "         1: 'Cluster 1', \n",
    "         2: 'Cluster 2'}\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y':y, 'label':labs['label']}) \n",
    "groups = df.groupby('label')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 13)) \n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=5,\n",
    "            color=colors[name],label=names[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')\n",
    "    ax.tick_params(axis= 'y',which='both',left='off',top='off',labelleft='off')\n",
    "    \n",
    "ax.legend()\n",
    "ax.set_title(plt_title)\n",
    "plt.savefig(plt_file)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "ax = plt.scatter(x, y, c=labs)\n",
    "plt.title('Clusters: Adobe MI')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e58888",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(zip(list(lab_df.columns), list(pickled_km.cluster_centroids_[0])))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587086f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = lab_df.columns\n",
    "c0 = list(pickled_km.cluster_centroids_[0])\n",
    "c1 = list(pickled_km.cluster_centroids_[1])\n",
    "c2 = list(pickled_km.cluster_centroids_[2])\n",
    "#c3 = list(pickled_km.cluster_centroids_[3])\n",
    "#c4 = list(pickled_km.cluster_centroids_[4])\n",
    "\n",
    "#d = dict((z[0], list(z[1:])) for z in zip(cols, c0, c1, c2, c3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07bdce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31823b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = cols.drop('label')\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_df = pd.DataFrame([c0,c1,c2], index = [0,1,2], columns=cols)\n",
    "#centroid_df[1] = km_model.cluster_centroids_[1]\n",
    "centroid_df\n",
    "\n",
    "fname = out_folder + 'demo_military_centroids_oct24.csv'\n",
    "fname\n",
    "centroid_df.to_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09bca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
